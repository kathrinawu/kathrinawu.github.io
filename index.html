<!DOCTYPE HTML>
<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yue Wu(吴玥)'s Homepage</title>

  <meta name="author" content="Yue Wu">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table
    style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Yue Wu(吴玥)</name>
                  </p>
                  <p style="text-align:center">
                    Email: wu.kathrina[at]gmail.com / <a href="https://scholar.google.com/citations?user=1xTR6qoAAAAJ&hl=en">Google Scholar</a> / <a href="https://yuewuhkust.github.io/Yue_Wu_CV.pdf">CV</a> / <a href="https://github.com/YueWuHKUST">Github</a>
                   /<a href="https://www.linkedin.com/in/yue-wu-%E5%90%B4%E7%8E%A5-60b212205/">Linkedin</a>
                     /<a href="https://openreview.net/profile?id=~Yue_Wu16">OpenReview</a>
                  </p>
                  <p>
                    I am currently a researcher at <strong>AI Theory Lab of Huawei Noah's Ark Lab</strong>  (Hong Kong), working closely with <a href="https://scholar.google.com/citations?user=XboZC1AAAAAJ&hl=en"> Dr. Zhenguo Li </a> and <a
                    href="https://xieenze.github.io/###"> Dr. Enze Xie </a>. I obtained my Ph.D. degree at Computer Science and
                      Engineering department of <strong>Hong Kong University of Science and Technology (HKUST)</strong> at June 2023 </a>,
                    supervised by <a href="https://cqf.io/"> Prof. Qifeng Chen </a>. Prior to this, I got my Bachelor's
                    degree from <a href="https://en.whu.edu.cn/"> Wuhan University </a> in 2018.
                  </p>  
                   
                  <p>
                    I have research experience in computational photography, image/video synthesis, 3D generative models, and neural rendering. And I'm conducting research in <strong>AIGC in 2D/3D/Video</strong>.
                  </p>
<!--                   <p style="color:red;">Always looking for research interns with strong CV/ML background, feel free to shoot an email if interested.</p> -->
                  <br>
                  <br>
                  <p>
                      <ul>
                        <li>News (Aug 2023): A paper is accepted to SIGGRAPH Asia 2023.</li>
                        <li>News (Aug 2023): I joined the AI Theory Lab of Huawei Noah's Ark Lab (Hong Kong). </li>                        
                        <li>News (June 2023): I passed my Ph.D. defense!</li>
                      </ul>
                      </p>



                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <img style="width:60%;max-width:60%" alt="profile photo" src="./yue_wu.jpg" class="hoverZoomLink"></a>
                </td>

              </tr>
            </tbody>
          </table>

          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Work Experience</heading>
                  <p>
                    Researcher, Huawei Noah's Ark Lab, Aug.2023 - now. 
                    <br>
                    Working closely with <a
                    href="https://scholar.google.com/citations?user=XboZC1AAAAAJ&hl=en"> Dr. Zhenguo Li </a> and <a
                    href="https://xieenze.github.io/###"> Dr. Enze Xie </a>.
                    <br>
                    <br> 
                    Research Intern, SenseTime, Jul.2017 - Dec.2017. 
                    <br>
                    Working with <a
                    href="https://scholar.google.com/citations?user=KZn9NWEAAAAJ&hl=zh-CN"> Dr. Wentao Liu </a> and <a
                    href="https://scholar.google.com/citations?user=AerkT0YAAAAJ&hl=en"> Dr. Chen Qian </a>.
                    <br>
                    <br> 
                    Research Intern, MSRA, Jan.2022 - May.2023. 
                    <br> 
                    Working with <a href="https://jlyang.org/"> Dr. Jiaolong Yang
                    </a>, <a href="https://scholar.google.com/citations?user=-ncz2s8AAAAJ&hl=en"> Dr. Fangyun Wei </a>
                    and <a href="https://www.microsoft.com/en-us/research/people/xtong/"> Dr. Xin Tong </a> in vision computing
                    group.
                  
                  </p>
                </td>
              </tr>
            </tbody>
          </table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>
               * indicates joint authors 
              </p>
            </td>
          </tr>
        </tbody>
        
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <tr></tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/efficiency.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle"> 
             
             <br>
              <papertitle> <strong><font color="#FF0000">PixArt-Σ</font></strong>: Weak-to-Strong Training of Diffusion Transformer for 4K Text-to-Image Generation.</papertitle>
              <br>
            Junsong Chen*, Chongjian Ge*, Enze Xie*†, <strong>Yue Wu*</strong>, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, Zhenguo Li
            <br>
              <em>ECCV</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2403.04692">[PDF]</a>
              <a href="https://github.com/PixArt-alpha/PixArt-sigma">[Code]</a>
              <a href=https://mp.weixin.qq.com/s/JcfVi0P4Db37McsfE6eXdg">[机器之心]</a>
              <br>
              <p></p>
          </td>
        </tr>

          
             <tr></tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/efficiency.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle"> 
             
             <br>
              <papertitle> <strong><font color="#FF0000">PIXART-δ</font></strong>: Fast and Controllable Image Generation with Latent Consistency Models.</papertitle>
              <br>
            Junsong Chen, <strong>Yue Wu</strong>, Simian Luo, Enze Xie, Sayak Paul, Ping Luo, Hang Zhao, Zhenguo Li
            <br>
              <em>Tech Report</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2401.05252">[PDF]</a>
              <br>
              <p></p>
          </td>
        </tr>
            
            
            
          <tr></tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/efficiency.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle"> 
             
             <br>
              <papertitle><strong><font color="#FF0000">PIXART-α</font></strong>: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis.</papertitle>
              <br>
              Junsong Chen*, Jincheng Yu*, Chongjian Ge*, Lewei Yao*, Enze Xie, <strong>Yue Wu</strong>, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, Zhenguo Li
              <br>
              <em>ICLR Spotlight</em>, 2024
              <br>
              <a href="https://pixart-alpha.github.io/static/pdfs/paper.pdf">[PDF]</a>
              <a href="https://arxiv.org/abs/2310.00426">[Arxiv]</a>
              <a href="https://pixart-alpha.github.io/">[Project]</a>
              <a href="https://github.com/PixArt-alpha/PixArt-alpha">[Code]</a>
              <a href="https://mp.weixin.qq.com/s/7dg6O5jmBwZ-1_LoiMvTlw">[机器之心]</a> 
              <br>
              <p></p>
          </td>
        </tr>
          
          <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/fastdit3d.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Editing Massive Concepts in Text-to-Image Diffusion Models.</papertitle>
                <br>
               Tianwei Xiong, Yue Wu, Enze Xie, <strong>Yue Wu</strong>, Zhenguo Li, Xihui Liu
                <br>
              <em>Preprint</em>, 2024
                <br>
                <a href="https://silentview.github.io/EMCID/">[Project]</a> 
                <br>
                <p></p>
            </td>
          </tr>

          <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/fastdit3d.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Automatic Controllable Colorization by Imagination.</papertitle>
                <br>
               Xiaoyan Cong, <strong>Yue Wu</strong>, Qifeng Chen, Chenyang Lei
                <br>
              <em>CVPR</em>, 2024
                <br>
                <br>
                <p></p>
            </td>
          </tr>
          
          <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/fastdit3d.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Fast Training of Diffusion Transformer with Extreme Masking for 3D Point Clouds Generation.</papertitle>
                <br>
                Shentong Mo, Enze Xie, <strong>Yue Wu</strong>, Junsong Chen, Matthias Nießner, Zhenguo Li
                <br>
              <em>ECCV</em>, 2024
                <br>
                <a href="https://dit-3d.github.io/FastDiT-3D/">[Project]</a> 
                <a href="https://arxiv.org/abs/2312.07231">[Arxiv]</a> 
                <br>
                <p></p>
            </td>
          </tr>

          
          <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/survey.jpg' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Reasoning with Foundation Models: Concepts, Methodologies, and Outlook.</papertitle>
                <br>
                Jiankai Sun, Chuanyang Zheng, Enze Xie, Zhengying Liu, Ruihang Chu, Jianing Qiu, Jiaqi Xu, Mingyu Ding, Hongyang Li, Mengzhe Geng, <strong>Yue Wu</strong>, Wenhai Wang, Junsong Chen, Xiaozhe Ren, Jie Fu, Junxian He, Wu Yuan, Qi Liu, Xihui Liu, Yu Li, Hao Dong, Yu Cheng, Ming Zhang, Pheng Ann Heng, Jifeng Dai, Ping Luo, Jingdong Wang, Jirong Wen, Xipeng Qiu, Yike Guo, Hui Xiong, Qun Liu, and Zhenguo Li
                <br>
              <em>Preprint</em>, 2023
                <br>
                <a href="https://github.com/reasoning-survey/Awesome-Reasoning-Foundation-Models">[Project]</a> 
                <a href="https://arxiv.org/abs/2312.11562">[Arxiv]</a> 
                <br>
                <p></p>
            </td>
          </tr>

          <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/dragavideo.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Drag-A-Video: Non-rigid Video Editing with Point-based Interaction.</papertitle>
                <br>
                Yao Teng, Enze Xie, <strong>Yue Wu</strong> , Haoyu Han, Zhenguo Li, Xihui Liu
                <br>
              <em>Preprint</em>, 2023
                <br>
                <a href="https://arxiv.org/abs/2312.02936">[Arxiv]</a>
                <a href="https://drag-a-video.github.io/">[Project]</a> 
                <br>
                <p></p>
            </td>
          </tr>
        
        

          
        <tr></tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <img src='images/AniPortraitGAN.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>AniPortraitGAN: Animatable 3D Portrait Generation from 2D Image Collections.</papertitle>
              <br>
              <strong>Yue Wu*</strong>, Sicheng Xu*, Jianfeng Xiang, Fangyun Wei, Qifeng Chen, Jiaolong Yang, Xin Tong
              <br>
              <strong><em>SIGGRAPH Asia</em></strong>, 2023
              <br>
              <a href="https://arxiv.org/abs/2309.02186">[PDF]</a>
              <a href="https://yuewuhkust.github.io/AniPortraitGAN/">[Project]</a>
              <a href="https://github.com/YueWuHKUST/AniPortraitGAN">[Code]</a>
              <a href="https://mp.weixin.qq.com/s/rPzYtAL7qfGkJ-jk6MZi9w">[新智元]</a>
              <br>
              <p></p>
          </td>
        </tr>
          
        <tr></tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one">
          <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: -5%">
                    <source src='images/anifacegan.mp4'>
          </video>
                    </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>AniFaceGAN: Animatable 3D-Aware Face Image Generation for Video Avatars</papertitle>
              <br>
              <strong>Yue Wu</strong>, Yu Deng, Jiaolong Yang, Fangyun Wei, Qifeng Chen, Xin Tong
              <br>
            <em>Neural Information Processing Systems</em> (<strong><em>NeurIPS</em></strong>) <strong><font color="#FF0000">(Spotlight)</font></strong>, 2022
              <br>
              <a href="https://arxiv.org/abs/2210.06465">[PDF]</a>
              <a href="https://yuewuhkust.github.io/AniFaceGAN/">[Project]</a>
              <a href="images/AniFaceGAN.txt">[BibTeX]</a>
              <a href="https://github.com/YueWuHKUST/AniFaceGAN/tree/main">[Code]</a>
              <br>
          </td>
      </tr>
      
      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/waterdrop.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Video Waterdrop Removal via Spatio-Temporal Fusion in Driving Scenes</papertitle>
              <br>
              Qiang Wen, <strong>Yue Wu</strong>, Qifeng Chen
              <br>
            <em>IEEE International Conference on Robotics and Automation</em> (<strong><em>ICRA</em></strong>), 2023
              <br>
              <a href="https://arxiv.org/abs/2302.05916">[PDF]</a>
              <a href="https://github.com/csqiangwen/Video_Waterdrop_Removal_in_Driving_Scenes">[Code]</a>
              <a href="https://mp.weixin.qq.com/s/Smu-E0yhxzS7_c0ahSCAdQ">[极市平台]</a>
              <br>
          </td>
      </tr>
      
      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='./images/long_term.jpg' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Improving Video Super-Resolution with Long-Term Self-Exemplars</papertitle>
              <br>
              Guotao Meng*, <strong>Yue Wu*</strong>, Qifeng Chen
              <br>
            <em>IEEE International Conference on Robotics and Automation</em> (<strong><em>ICRA</em></strong>), 2023
              <br>
              <!-- <a href="https://cqf.io/papers/Embedding_Novel_Views_ICCV2021.pdf">[PDF]</a> -->
              <a href="https://arxiv.org/abs/2106.12778">[arXiv]</a>
              <!-- <a href="https://yuewuhkust.github.io/iccv-2021-embedding/">[Project]</a> -->
              <!-- <a href="images/GRAM-HD.txt">[BibTeX]</a> -->
              <br>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
  <video playsinline autoplay loop preload muted style="width:100%;max-width:100%; position: absolute;top: 10%">
              <source src='./OVP_VFI/all.mp4'>
  </video>
        </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Optimizing Video Prediction via Video Frame Interpolation</papertitle>
              <br>
              <strong>Yue Wu</strong>, Qiang Wen, Qifeng Chen
              <br>
            <em>IEEE Conference on Computer Vision and Pattern Recognition</em> (<strong><em>CVPR</em></strong>), 2022
              <br>
              <a href="https://arxiv.org/abs/2206.13454">[PDF]</a>
              <a href="https://yuewuhkust.github.io/OVP_VFI/">[Project]</a>
              <a href="https://github.com/YueWuHKUST/CVPR2022-Optimizing-Video-Prediction-via-Video-Frame-Interpolation/">[Code and Results]</a>
              <a href="images/OVP_VFI.txt">[BibTeX]</a>
              <br>
          </td>
      </tr>
      
      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
              <img src='images/novel.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Embedding Novel Views in a Single JPEG Image</papertitle>
              <br>
              <strong>Yue Wu*</strong>, Guotao Meng*, Qifeng Chen
              <br>
            <em>International Conference on Computer Vision</em> (<strong><em>ICCV</em></strong>), 2021
              <br>
              <a href="https://cqf.io/papers/Embedding_Novel_Views_ICCV2021.pdf">[PDF]</a>
              <a href="https://arxiv.org/abs/2108.13003">[arXiv]</a>
              <a href="https://yuewuhkust.github.io/iccv-2021-embedding/">[Project]</a>
              <a href="images/embedding.txt">[BibTeX]</a>
              <br>
          </td>
      </tr>
      
      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/colorization.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Towards Photorealistic Colorization by Imagination</papertitle>
              <br>
              Chenyang Lei *, <strong>Yue Wu*</strong>, Qifeng Chen
              <br>
              <em>Preprint</em> 2021,
              <br>
              <!-- <a href="https://cqf.io/papers/Embedding_Novel_Views_ICCV2021.pdf">[PDF]</a> -->
              <a href="https://arxiv.org/abs/2108.09195">[arXiv]</a>
              <!-- <a href="https://yuewuhkust.github.io/iccv-2021-embedding/">[Project]</a> -->
              <!-- <a href="images/GRAM-HD.txt">[BibTeX]</a> -->
              <br>
          </td>
      </tr>

      

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='./images/video_pred.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Future Video Synthesis with Object Motion Prediction</papertitle>
              <br>
              <strong>Yue Wu</strong>, Rongrong Gao, Jaesik Park, Qifeng Chen
              <br>
            <em>Computer Vision and Pattern Recognition Conference</em> (<strong><em>CVPR</em></strong>), 2020
              <br>
              <a href="https://cqf.io/papers/Future_Video_Synthesis_With_Object_Motion_Prediction_CVPR2020.pdf">[PDF]</a>
              <a href="https://arxiv.org/abs/2004.00542">[arXiv]</a>
              <a href="https://github.com/YueWuHKUST/FutureVideoSynthesis">[Code and Result]</a>
              <a href="./images/cvpr2020_videopred.txt">[BibTeX]</a>
              
              <!-- <a href="https://yuewuhkust.github.io/iccv-2021-embedding/">[Project]</a> -->
              <!-- <a href="images/GRAM-HD.txt">[BibTeX]</a> -->
              <br>
          </td>
      </tr>

      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='./images/pose.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Towards Multi-Person Pose Tracking: Bottom-up and Top-down Methods</papertitle>
              <br>
              Sheng Jin,
                    Xujie Ma,
                    Zhipeng Han,
                    <strong>Yue Wu</strong>,
                    Wei Yang,
                    Wentao Liu,
                    Chen Qian,
                    Wanli Ouyang
              <br>
            <em>International Conference on Computer Vision</em> (<strong><em>ICCV Workshops</em></strong>), 2017
              <br>
              <a href="https://jin-s13.github.io/papers/BUTD.pdf">[PDF]</a>
              <!-- <a href="https://arxiv.org/abs/2004.00542">[arXiv]</a>
              <a href="https://github.com/YueWuHKUST/FutureVideoSynthesis">[Code and Result]</a>
               -->
              <!-- <a href="https://yuewuhkust.github.io/iccv-2021-embedding/">[Project]</a> -->
              <!-- <a href="images/GRAM-HD.txt">[BibTeX]</a> -->
              <br>
              <p>Ranked 2nd Places in ICCV Posetrack Challenge</p>
          </td>
      </tr>           
            
      <tr></tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='./images/icme.png' style="width:100%;max-width:100%; position: absolute;top: 10%">
              </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
              <papertitle>Saliency map generation based on saccade target theory</papertitle>
              <br>
              <strong>Yue Wu</strong>,
                    Zhenzhong Chen
              <br>
            <strong><em>ICME</em></strong>, 2017
              <br>
              <a href="https://ieeexplore.ieee.org/document/8019456">[PDF]</a>
              <!-- <a href="https://arxiv.org/abs/2004.00542">[arXiv]</a>
              <a href="https://github.com/YueWuHKUST/FutureVideoSynthesis">[Code and Result]</a>
               -->
              <!-- <a href="https://yuewuhkust.github.io/iccv-2021-embedding/">[Project]</a> -->
              <!-- <a href="images/GRAM-HD.txt">[BibTeX]</a> -->
              <br>
              <!-- <p>Ranked 2nd Places in ICCV Posetrack Challenge</p> -->
          </td>
      </tr>                    
  


<!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Academic Services</heading>
        <p>
          Conference: ECCV, CVPR, ICRA 
          <br>
          <br>
        </p>
      </td>
    </tr>
  </tbody>
</table>           
   -->

           
            
                   
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Teaching</heading>
        <p>
          COMP5411: Computer Graphics, 2019
          <br>
          COMP2711H: Honors Discrete Mathematical Tools for Computer Science, 2020
          <br>
          COMP3511: Operating System, 2021
          <br>
          COMP5214: Advanced Deep Learning Architectures, 2022
        </p>
      </td>
    </tr>
  </tbody>
</table>



<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Honors and Awards</heading>
        <p>
          HKUST RedBird Academic Excellence Award, HKUST
          <br>
          Postgraduate Scholarship, HKUST
          <br>
          National scholarship, Wuhan University
          <br>
          First-Class scholarship, Wuhan University
          <br>
          Best Head Movement Prediction Student Prize ICME Grand Challenge Salient360! 2017
          <br>
          Meritorious Winner Interdisciplinary Contest In Modeling (ICM) 2017
          <br>
        </p>
      </td>
    </tr>
  </tbody>
</table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Academic Service</heading>
        <p>
          Reviewer for ECCV 2022, CVPR 2022/2023/2024, ICRA 2023, JMLR
          <br>
          <br>
        </p>
      </td>
    </tr>
  </tbody>
</table>

<a href="https://info.flagcounter.com/UQVZ"><img src="https://s11.flagcounter.com/count2/UQVZ/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_0/flags_0/percent_0/" alt="Flag Counter" border="0"></a>

</html>
